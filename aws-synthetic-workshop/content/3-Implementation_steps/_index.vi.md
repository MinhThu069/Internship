---
title : "CÃ¡c bÆ°á»›c thá»±c hiá»‡n"
date: 2025-07-16
weight : 3
chapter : false
pre : " <b> 3. </b> "
---

## ğŸ› ï¸ CÃ¡c bÆ°á»›c thá»±c hiá»‡n

Pháº§n nÃ y hÆ°á»›ng dáº«n tá»«ng bÆ°á»›c cá»¥ thá»ƒ Ä‘á»ƒ triá»ƒn khai **pipeline táº¡o dá»¯ liá»‡u tá»•ng há»£p cÃ³ báº£o toÃ n quyá»n riÃªng tÆ°** trÃªn AWS.
Pipeline sáº½ bao gá»“m: **lÆ°u trá»¯ â€“ tiá»n xá»­ lÃ½ â€“ kiá»ƒm tra PII â€“ huáº¥n luyá»‡n mÃ´ hÃ¬nh TVAE â€“ Ä‘Ã¡nh giÃ¡ dá»¯ liá»‡u**.

## 1. Upload dá»¯ liá»‡u lÃªn Amazon S3
ğŸŒŸ Má»¥c tiÃªu:
ÄÆ°a táº­p dá»¯ liá»‡u CSV gá»‘c lÃªn S3 Ä‘á»ƒ Ä‘á»ƒ lÃ m nguá»“n Ä‘áº§u vÃ o.

ğŸ“‚ Thá»±c hiá»‡n:
- Truy cáº­p **AWS Console** â†’ chá»n **Amazon S3**.
- Táº¡o má»™t bucket má»›i, vÃ­ dá»¥: `synthetic-data-demo`.
- Upload táº­p `bs140513_032310.csv` lÃªn thÆ° má»¥c raw/
Hoáº·c sá»­ dá»¥ng:
```
aws s3 mb s3://synthetic-data-demo
aws s3 cp bs140513_032310.csv s3://synthetic-data-demo/raw/
```

ğŸ’¬ LÆ°u Ã½:
Bucket nÃªn cÃ¹ng Region vá»›i SageMaker (vÃ­ dá»¥ ap-southeast-1)
Dá»¯ liá»‡u gá»‘c khÃ´ng nÃªn chá»©a thÃ´ng tin Ä‘á»‹nh danh tháº­t

## 2. Tiá»n xá»­ lÃ½ dá»¯ liá»‡u vá»›i AWS Glue
ğŸŒŸ Má»¥c tiÃªu:
LÃ m sáº¡ch dá»¯ liá»‡u, chuáº©n hÃ³a schema, xá»­ lÃ½ giÃ¡ trá»‹ thiáº¿u trÆ°á»›c khi huáº¥n luyá»‡n.
```python
# glue_job_banksim.py
import sys
from awsglue.transforms import ApplyMapping
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from awsglue.context import GlueContext
from awsglue.job import Job

args = getResolvedOptions(sys.argv, ["JOB_NAME", "SOURCE_BUCKET", "TARGET_BUCKET"])
sc = SparkContext()
glueContext = GlueContext(sc)
spark = glueContext.spark_session
job = Job(glueContext)
job.init(args["JOB_NAME"], args)

source_path = f"s3://{args['SOURCE_BUCKET']}/raw/"
target_path = f"s3://{args['TARGET_BUCKET']}/cleaned/"

# Äá»c CSV (header auto)
dyf = glueContext.create_dynamic_frame.from_options(
    connection_type="s3",
    connection_options={"paths": [source_path]},
    format="csv",
    format_options={"withHeader": True}
)

mappings = []
mappings.append(("step", "long", "step", "int"))
mappings.append(("amount", "double", "amount", "double"))

try:
    dyf_cleaned = ApplyMapping.apply(frame=dyf, mappings=mappings)
except Exception:
    dyf_cleaned = dyf

# Ghi ra Parquet
glueContext.write_dynamic_frame.from_options(
    frame=dyf_cleaned,
    connection_type="s3",
    connection_options={"path": target_path},
    format="parquet"
)

job.commit()

```
ğŸ’¬ LÆ°u Ã½:
- Giá»¯ schema nháº¥t quÃ¡n Ä‘á»ƒ trÃ¡nh lá»—i khi huáº¥n luyá»‡n.


## 3. QuÃ©t vÃ  loáº¡i bá» thÃ´ng tin Ä‘á»‹nh danh (PII) vá»›i Amazon Macie
ğŸŒŸ Má»¥c tiÃªu:
Äáº£m báº£o dá»¯ liá»‡u Ä‘Ã£ lÃ m sáº¡ch khÃ´ng cÃ²n chá»©a email, sá»‘ tÃ i khoáº£n, Ä‘á»‹a chá»‰â€¦
```python
import boto3, json, time, os
region = os.environ.get("AWS_REGION","ap-southeast-1")
bucket = os.environ.get("BUCKET") or "synthetic-data-demo-<your-unique-suffix>"

macie = boto3.client("macie2", region_name=region)

resp = macie.create_classification_job(
    jobType="ONE_TIME",
    name="macie-scan-cleaned",
    s3JobDefinition={
        "bucketDefinitions":[{"accountId": boto3.client('sts').get_caller_identity()['Account'], "buckets":[bucket]}],
        "scoping":{
            "includes":{
                "and":[{"simpleScopeTerm":{"comparator":"STARTS_WITH","key":"OBJECT_KEY","values":["cleaned/"]}}]
            }
        }
    },
    samplingPercentage=100
)
print("Created Macie job:", resp["jobId"])

```
ğŸ’¬ LÆ°u Ã½:
- Náº¿u váº«n cÃ²n PII â†’ quay láº¡i bÆ°á»›c Glue Ä‘á»ƒ loáº¡i bá».

## 4. Huáº¥n luyá»‡n mÃ´ hÃ¬nh TVAE trong SageMaker Studio Lab
ğŸŒŸ Má»¥c tiÃªu:
Sinh dá»¯ liá»‡u tá»•ng há»£p giá»¯ nguyÃªn phÃ¢n phá»‘i thá»‘ng kÃª nhÆ°ng khÃ´ng chá»©a PII.
- ğŸ”„ CÃ¡c bÆ°á»›c:
1. Má»Ÿ Jupyter Notebook trong SageMaker Studio Lab.
2. CÃ i cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t:
``` 
pip install sdv[all] pandas matplotlib seaborn boto3 
```
3. Táº£i Parquet Ä‘Ã£ lÃ m sáº¡ch tá»« S3 & Ä‘á»c vÃ o Pandas
```python
import boto3, os, pandas as pd

AWS_REGION = os.environ.get("AWS_REGION","ap-southeast-1")
BUCKET = os.environ.get("BUCKET") or "synthetic-data-demo-<your-unique-suffix>"
CLEANED_PREFIX = "cleaned/"

s3 = boto3.client("s3", region_name=AWS_REGION)

# Táº£i file parquet Ä‘áº§u tiÃªn vá» (demo)
resp = s3.list_objects_v2(Bucket=BUCKET, Prefix=CLEANED_PREFIX)
keys = [o["Key"] for o in resp.get("Contents", []) if o["Key"].endswith(".parquet")]
assert len(keys) > 0, "KhÃ´ng tÃ¬m tháº¥y file Parquet trong cleaned/"

local_pq = "cleaned.parquet"
s3.download_file(BUCKET, keys[0], local_pq)

# Äá»c parquet => pandas
df = pd.read_parquet(local_pq)
df.head()
```

4. Huáº¥n luyá»‡n TVAE & sinh dá»¯ liá»‡u tá»•ng há»£p
```python
from sdv.tabular import TVAE
import numpy as np
import random

SEED = 42
np.random.seed(SEED); random.seed(SEED)

model = TVAE(epochs=300, batch_size=500, cuda=False)  # cuda=True náº¿u cÃ³ GPU
model.fit(df)

synthetic_df = model.sample(len(df))
synthetic_df.head()
```

## 5. ÄÃ¡nh giÃ¡ dá»¯ liá»‡u tá»•ng há»£p
ğŸŒŸ Má»¥c tiÃªu:
So sÃ¡nh phÃ¢n phá»‘i dá»¯ liá»‡u tá»•ng há»£p vá»›i dá»¯ liá»‡u gá»‘c.
```python
import matplotlib.pyplot as plt

def compare_hist(col):
    if col not in df.columns or col not in synthetic_df.columns:
        return
    plt.figure(figsize=(6,4))
    df[col].dropna().astype(float).hist(bins=30, alpha=0.5, label="Real")
    synthetic_df[col].dropna().astype(float).hist(bins=30, alpha=0.5, label="Synthetic")
    plt.legend(); plt.title(f"Distribution: {col}"); plt.show()

# Thá»­ vÃ i cá»™t numeric â€œquen thuá»™câ€
for col in ["amount", "step"]:
    if col in df.columns:
        compare_hist(col)

# Correlation (náº¿u lÃ  numeric)
real_corr = df.select_dtypes(include="number").corr()
syn_corr  = synthetic_df.select_dtypes(include="number").corr()
print("Real Corr shape:", real_corr.shape, "Synthetic Corr shape:", syn_corr.shape)
```

## 6. LÆ°u trá»¯ vÃ  tÃ¡i sá»­ dá»¥ng dá»¯ liá»‡u tá»•ng há»£p lÃªn S3
ğŸŒŸ Má»¥c tiÃªu:
LÆ°u dá»¯ liá»‡u tá»•ng há»£p Ä‘á»ƒ chia sáº» hoáº·c huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh khÃ¡c.
```
synthetic_df.to_csv("synthetic_banksim.csv", index=False)
s3.upload_file("synthetic_banksim.csv", "synthetic-data-demo", "synthetic/synthetic_banksim.csv")

```

## 7. Cleanup (tuá»³ chá»n)

```bash
# XoÃ¡ má»i object trong bucket
aws s3 rm s3://$BUCKET --recursive

# XoÃ¡ bucket
aws s3 rb s3://$BUCKET

# XoÃ¡ Glue Job
aws glue delete-job --job-name Glue-BankSim-Clean

# (Náº¿u muá»‘n) Detach & xoÃ¡ role Glue
aws iam detach-role-policy --role-name AWSGlueServiceRole-SyntheticDemo --policy-arn arn:aws:iam::aws:policy/service-role/AWSGlueServiceRole
aws iam detach-role-policy --role-name AWSGlueServiceRole-SyntheticDemo --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess
aws iam delete-role --role-name AWSGlueServiceRole-SyntheticDemo

```

{{% notice tip %}}
LÆ°u Ã½ quan trá»ng:
- Giá»¯ nguyÃªn Region (vÃ­ dá»¥ ap-southeast-1) cho S3/Glue/Macie/Studio Ä‘á»ƒ trÃ¡nh lá»—i quyá»n & Ä‘á»‹nh vá»‹.
- TÃªn bucket pháº£i unique toÃ n cáº§u; sá»­a biáº¿n BUCKET ngay tá»« Ä‘áº§u.
- Macie: enable service trong region trÆ°á»›c khi táº¡o job. Báº£n dÃ¹ng thá»­ thÆ°á»ng miá»…n phÃ­ 30 ngÃ y.
- SDV/TVAE: láº§n Ä‘áº§u cÃ i cÃ³ thá»ƒ hÆ¡i lÃ¢u (kÃ©o theo torch/sklearn). Nhá»› Restart Kernel sau khi pip install.
- Schema BankSim cÃ³ thá»ƒ khÃ¡c nhau (báº£n PaySim/BankSim biáº¿n thá»ƒ). MÃ¬nh Ä‘Ã£ trÃ¡nh â€œcá»©ngâ€ schema; náº¿u báº¡n muá»‘n Ã©p kiá»ƒu nhiá»u cá»™t hÆ¡n trong Glue, chá»‰nh mappings tÆ°Æ¡ng á»©ng (hoáº·c Ä‘á»c schema tá»« crawler rá»“i export).
{{% /notice %}}